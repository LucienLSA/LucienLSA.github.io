<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>强化学习算法发展与介绍 | Lucien</title><meta name="author" content="Shenao Lu,shenaolu0603@163.com"><meta name="copyright" content="Shenao Lu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="强化学习算法介绍 视频来源 基础介绍 马尔可夫链 S A R三个元素  智能体在环境中观测到状态S 状态S被输入到智能体，经过计算，选择动作A 动作A使智能体进入另一个状态S，并返回奖励R给智能体 智能体根据返回调整自己策略，重复以上步骤  不确定性：（智能体的选择、环境的不确定性）  选择过程，智能体选择会影响到下一个状态，不同动作之间选择即称为智能体的策略 环境随机性，智能体无法控制，acti">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习算法发展与介绍">
<meta property="og:url" content="https://lucienlsa.github.io/2024/04/29/RL/RL%E6%95%99%E7%A8%8Bb%E7%AB%99/index.html">
<meta property="og:site_name" content="Lucien">
<meta property="og:description" content="强化学习算法介绍 视频来源 基础介绍 马尔可夫链 S A R三个元素  智能体在环境中观测到状态S 状态S被输入到智能体，经过计算，选择动作A 动作A使智能体进入另一个状态S，并返回奖励R给智能体 智能体根据返回调整自己策略，重复以上步骤  不确定性：（智能体的选择、环境的不确定性）  选择过程，智能体选择会影响到下一个状态，不同动作之间选择即称为智能体的策略 环境随机性，智能体无法控制，acti">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2024/04/05/Ivzr23MiNpQftUF.jpg">
<meta property="article:published_time" content="2024-04-29T01:52:33.389Z">
<meta property="article:modified_time" content="2024-05-22T10:52:31.720Z">
<meta property="article:author" content="Shenao Lu">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/04/05/Ivzr23MiNpQftUF.jpg"><link rel="shortcut icon" href="https://s2.loli.net/2024/04/03/hJcDCjPRvtpqdWs.png"><link rel="canonical" href="https://lucienlsa.github.io/2024/04/29/RL/RL%E6%95%99%E7%A8%8Bb%E7%AB%99/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '强化学习算法发展与介绍',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-22 18:52:31'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2024/04/04/oghnOyU3GQpPtKL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2024/04/05/Ivzr23MiNpQftUF.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Lucien"><span class="site-name">Lucien</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">强化学习算法发展与介绍</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-29T01:52:33.389Z" title="发表于 2024-04-29 09:52:33">2024-04-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-22T10:52:31.720Z" title="更新于 2024-05-22 18:52:31">2024-05-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/reinforcement-learning/">reinforcement learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="强化学习算法发展与介绍"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>强化学习算法介绍</h1>
<h2 id="视频来源">视频来源</h2>
<h2 id="基础介绍">基础介绍</h2>
<h2 id="马尔可夫链">马尔可夫链</h2>
<p>S A R三个元素</p>
<ol>
<li>智能体在环境中观测到状态S</li>
<li>状态S被输入到智能体，经过计算，选择动作A</li>
<li>动作A使智能体进入另一个状态S，并返回奖励R给智能体</li>
<li>智能体根据返回调整自己策略，重复以上步骤</li>
</ol>
<p>不确定性：（智能体的选择、环境的不确定性）</p>
<ul>
<li>选择过程，智能体选择会影响到下一个状态，不同动作之间选择即称为智能体的策略</li>
<li>环境随机性，智能体无法控制，action返回新的状态或奖励也可能不同</li>
</ul>
<h2 id="Q值和V值">Q值和V值</h2>
<p>奖励有正有负，作为智能体学习的引导，期望智能体获得更多奖励。长远看待，将未来奖励计算到当前状态</p>
<ul>
<li>评估动作价值称为Q值，代表智能体选择动作后，直到最终状态奖励总和的期望</li>
<li>评估状态价值称为V值，代表智能体状态下直到最终奖励总和的期望</li>
</ul>
<p>价值越高，当前状态到最终状态的平均奖励越高，当前状态选择价值高的动作</p>
<h3 id="V值">V值</h3>
<p>从某个状态按照策略，走到最终状态很多很多次；最终获得奖励总和的平均值</p>
<h3 id="Q值">Q值</h3>
<p>从某个状态选取动作A，走到最终状态很多很多次，最终得到奖励总和的平均值</p>
<p>Q值和策略没有直接相关，而与环境的状态转移概率相关，环境的状态转移概率是不变的</p>
<h3 id="V值和Q值">V值和Q值</h3>
<p>Q值和V值都是马克洛夫数上的节点；</p>
<p>价值评价的方式一样，从当前节点出发，一直走到最终节点，所有的奖励的期望值</p>
<ul>
<li>V是子节点的Q的期望，V值和策略相关</li>
<li>Q是子节点的V的期望，需要将R计算在内</li>
</ul>
<h2 id="蒙特卡洛采样">蒙特卡洛采样</h2>
<ol>
<li>根据策略往前走，走到最后，期间记录每一个状态转移，和获得的奖励r</li>
<li>从终点往前走，一遍一遍计算G值，G值等于上一个状态的G值(G’)乘以一定折扣(gamma)，再加上奖励r</li>
</ol>
<p>折扣率为超参数，折扣率将未来很多步奖励，折算到当前节点。</p>
<p>G值：某个状态到最终状态的奖励总和。</p>
<p>V值：</p>
<ul>
<li>某个状态下，通过影分身到达最终状态，所有影分身获得的奖励的平均值（回溯到这个状态，每条轨迹下都有一个G值，对其求平均）。</li>
<li>V值是G值的平均。V值和策略是相关的。</li>
</ul>
<h3 id="蒙特卡洛缺点">蒙特卡洛缺点</h3>
<p>每次都需要从头到尾，再进行回溯更新，如果最终状态很难到达，则需要每一次转很久才能更新一次G值</p>
<h2 id="蒙特卡洛估算状态V值">蒙特卡洛估算状态V值</h2>
<h3 id="GBDT算法-梯度提升决策树">GBDT算法(梯度提升决策树)</h3>
<p>构造一组弱的学习器(树)，把多颗决策树的结果累加起来作为最终的预测输出</p>
<h4 id="Boosting核心思想">Boosting核心思想</h4>
<ol>
<li>训练基分类器采用串行，各基分类器之间有依赖，将基分类器层层叠加，每一层训练时对前一层基分类器分错的样本，给予更高权重。根据各层分类器的结果的加权得到最终结果。</li>
</ol>
<p>Bagging采用并行训练，各基分类器之间无依赖。</p>
<h4 id="GBDT原理">GBDT原理</h4>
<ol>
<li>所有弱分类器的结果相加等于预测值</li>
<li>每次都以当前预测为基准，下一个弱分类器去拟合误差函数对预测值的残差（预测值与真实值之间的误差）</li>
<li>GBDT的弱分类器使用的是树模型</li>
</ol>
<h4 id="负梯度近似误差">负梯度近似误差</h4>
<p>损失函数为均方误差损失函数，每次拟合真实值-预测值，即残差</p>
<p><img src="https://s2.loli.net/2024/05/17/d6mWZnUrzMpJtAK.png" alt="image.png"></p>
<p>损失函数的负梯度近似误差</p>
<p><img src="https://s2.loli.net/2024/05/17/EaXP4TrCQeLWkVR.png" alt="image.png"></p>
<p>GBDT将多棵树的得分累加得到最终预测的得分，且每轮迭代，在现有树的基础上，增加一棵新的树去拟合前面的树的预测值与真实值之间的残差。</p>
<h4 id="梯度提升与梯度下降">梯度提升与梯度下降</h4>
<p>都是利用损失函数的负梯度方向信息，更新当前模型</p>
<ol>
<li>梯度下降，模型以参数化形式</li>
<li>梯度提升，模型不需以参数化形式，直接定义在函数空间中</li>
</ol>
<h3 id="新平均-旧平均-步长-新加入元素-旧平均">新平均=旧平均+步长*(新加入元素-旧平均)</h3>
<p><img src="https://s2.loli.net/2024/05/17/4BpzZxNEoslGJQ6.png" alt="image.png"></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi>α</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>t</mi></msub><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>α</mi><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">+\alpha(G_{t}-V(S_t)) = -\alpha(y-\hat y) = \alpha(\hat y - y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">+</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></p>
<h2 id="TD-时序差分-估算状态V值">TD(时序差分)估算状态V值</h2>
<p>在蒙特卡洛方法上改进</p>
<ol>
<li>TD算法只需要走N步，开始回溯更新</li>
<li>先走N步，每经过一个状态，把奖励记录下来，开始回溯</li>
<li>V值计算。假设在N步之后，到达最终状态。假设“最终状态”之前没有走过，状态为0；假设“最终状态”已经走过，状态V值为当前值。再开始回溯</li>
</ol>
<h3 id="TD估算V值">TD估算V值</h3>
<p>蒙特卡洛中，G是更新目标，TD中，r+gamma*V是更新目标</p>
<p><img src="https://s2.loli.net/2024/05/17/OZbR97P6jE5xfh4.png" alt="image.png"></p>
<h2 id="SARSA算法">SARSA算法</h2>
<p>使用TD预估Q值，智能体看作一个Qfunction，输入为St和At。</p>
<h3 id="SARSA思想">SARSA思想</h3>
<p>同一个策略下产生的动作A的Q值替代V(St+1)</p>
<p><img src="https://s2.loli.net/2024/05/17/bXF5KEWxmaQ3YPH.png" alt="image.png"></p>
<p>与TD方法估算V值相比，SARSA将V换成Q</p>
<h3 id="SARSA-LambdaTable">SARSA LambdaTable</h3>
<p>引入Lambda=trace_decay</p>
<p>引入eligibility_trace记录对应观测状态state对应的action位置，仅仅记录轨迹。而不是Qtable中记录的Q值。但两者的形状是同步一致的。</p>
<h2 id="Qlearning算法">Qlearning算法</h2>
<p>选择能获得最大Q值的动作，用所有动作的Q值的最大值替代V(St+1)</p>
<p>Qlearning与SARSA差一个max</p>
<h2 id="Q-table">Q table</h2>
<p>初始化Q表的矩阵，通过Qlearning学习调整，反复迭代最优解Qtable。在未来预测智能体时，根据不同的动作得到Q值（查表），选择最大值得动作，作为智能体得下一步动作。</p>
<h2 id="DQN">DQN</h2>
<p>Qtable适合离散得格子游戏，但实际很多状态是连续的。Qtable作用找S-A的对应关系，函数取连续状态，则F(S)=A</p>
<p>DQN用神经网络函数代替Qtable</p>
<p><img src="https://s2.loli.net/2024/05/17/isP1vZ3nYImjyAC.png" alt="image.png"></p>
<p>DQN的实现</p>
<ol>
<li>执行A，往前一步，到达St+1</li>
<li>St+1输入Q网络，计算St+1所有动作的Q值</li>
<li>获得最大的Q值加上奖励R作为更新目标(R+maxQ(St+1))</li>
<li>计算损失-Q(S,A)相当于有监督学习中拟合的机率</li>
<li>maxQ(St+1)+R相当于有监督学习中标签labels,y_true</li>
<li>MSE函数，得到两者的loss</li>
<li>用损失更新Q网络</li>
</ol>
<p><img src="https://s2.loli.net/2024/05/17/4PfyiMxlNOmoZ5a.png" alt="DQN"></p>
<h2 id="贪婪Greedy">贪婪Greedy</h2>
<p>Epsilon Greedy  (epsilon在学习过程中进行衰减)</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>a</mi></msub><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">argmax_{a}Q(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>，概率1-epsilon；<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">random</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">m</span></span></span></span>，概率epsilon</p>
<h2 id="回放缓存（经验回放）Replay-Buffer">回放缓存（经验回放）Replay Buffer</h2>
<p>每一步s，选择a，进入新的状态s’，获得的奖励r，新状态是否为终止状态。将数据全部存放在回放缓存</p>
<p>设定batch size，抽取一个batch的数据，将其放入网络进行训练。训练之后继续，把新产生的数据添加到回放缓存中。即以前产生的数据同样用来训练数据。</p>
<p>经验回放使训练更高效，也减少训练产生的过度拟合问题。</p>
<p>off-policy，学习数据不是来于正在训练的，而是有一些采样来自之前模型产生的数据</p>
<h2 id="Fixed-Q-targets">Fixed Q-targets</h2>
<p>DQN目标：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>+</mo><mi>γ</mi><mo>∗</mo><mi>m</mi><mi>a</mi><mi>x</mi><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R+\gamma * maxQ(s&#x27;,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>，但Q网络为更新和调整的参数，则会造成Q网络学习效率低，且不稳定</p>
<p>采用两个Q网络：一个原来的Q网络用于估算Q(s)，另一个targetQ网络，targetQ网络自己不会更新，它在更新过程中是固定的，用于计算更新目标。</p>
<p>N次更新后，将新Q的参数赋值给旧Q</p>
<p><img src="https://s2.loli.net/2024/05/17/sJgkQ9UvBH4G1xi.png" alt="image.png"></p>
<h2 id="Double-DQN">Double DQN</h2>
<p>由于DQN估计的Q值往往会偏大，即高估Q值。</p>
<p>DQN中的，target(R+maxQ(St+1,a))和Q(S,A)计算损失loss，得到Q网络的更新，其相当于自我监督；</p>
<p><img src="https://s2.loli.net/2024/05/18/FgCWfAsL1yt9GRJ.png" alt="image.png"></p>
<p>Double DQN用两个Q网络(Q,Q’)，选取评估较小的值计算目标，避免Q值的过高。（但如果Q’也高估，则该动作不能用于Q值更新）</p>
<p>Q1网络能推荐获得最大Q值的动作，Q2网络计算这个动作在Q2网络中的Q值，如果Q2网络的Q值没有被高估，则为恰当的值。恰好用Fixed Q-targets，则表示两个网络</p>
<h2 id="Dueling-DQN">Dueling DQN</h2>
<p>原DQN直接预估Q值：需要更新某个动作Q值，直接更新Q网络，Q值提升</p>
<p>Dueling DQN 预估S值和A值：网络更新时，由于A值之和必须为0的限制，网络会优先更新S值，S值是Q值的平均数，平均数调整相当于一次性S下所有Q值都进行更新。则网络更新时，不仅更新某个动作的Q值，而且在这个状态下，所有动作的Q值都调整一次。以更少的次数进行更多Q值的更新</p>
<p><img src="https://s2.loli.net/2024/05/17/HisUNbFQcYRnGZd.png" alt="image.png"></p>
<p>Dueling DQN中优先调整S值，但最终Target目标没有变。</p>
<h2 id="Prioritized-Experience-Replay">Prioritized Experience Replay</h2>
<p>预测不好的data，通过Q网络选择不好的action，其被称为困难样本。困难样本会有更大的TD误差，具有更高被采样的可能性去学习，放在Experience Buffer中学习困难样本。</p>
<h2 id="Multi-step">Multi-step</h2>
<p>蒙特卡洛和时序差分之间平衡，对TD来说等于往后多走了几步，方差会比较大。但不能多走太多，可能会导致reward的加和问题。</p>
<h2 id="Noise-Net">Noise Net</h2>
<h3 id="Noise-on-Action-Epsilon-Greedy">Noise on Action(Epsilon Greedy)</h3>
<p>随机地尝试动作：相同状态，智能体采取不同的动作</p>
<h3 id="Noise-on-Parameters">Noise on Parameters</h3>
<p>有系统地尝试动作：相同或相似状态，智能体采取相同动作，以状态依赖形式探索，更具有针对性地探索。</p>
<p>Noise Net则将噪声加入到每一步开始地Qfunction参数中，且每一步噪声不会改变</p>
<h2 id="Distributional-Q-function">Distributional Q-function</h2>
<p>原来Q值为期望值，不同分布可能会有同样的期望，如果选择期望较大时，其可能对应方差恰巧很大，则action会风险很高。</p>
<h2 id="Continuous-Actions">Continuous Actions</h2>
<p>action选取为连续的值。</p>
<h3 id="Solution1">Solution1</h3>
<p>采样一组动作，寻找使得Q值最大的动作（采样时即进行离散化）</p>
<h3 id="Solution2">Solution2</h3>
<p>梯度下降求解最优化问题</p>
<h3 id="Solution3">Solution3</h3>
<p>设计神经网络使最优化更简单，网络调参，收敛时得到更好的u</p>
<p><img src="https://s2.loli.net/2024/05/18/EhNPBdYuevAnUOK.png" alt="image.png"></p>
<h2 id="Value-based-and-Policy-based">Value-based and Policy-based</h2>
<ol>
<li>value-based 利用reward奖励，在某些状态下选择好的行为将Q值提高或降低</li>
<li>policy-based 利用reward奖励，网络调参，在选择某种行为的概率提高或降低。即若智能体动作正确，则让这个动作获得更多被选择的概率。</li>
</ol>
<h2 id="策略梯度-PG">策略梯度(PG)</h2>
<h3 id="基本思想">基本思想</h3>
<p>直接训练神经网络输入state输出action，该过程不用计算Q值</p>
<h3 id="策略梯度原理">策略梯度原理</h3>
<p>传统使用Q table查表或Q网络通过Critic评判，以选择action。</p>
<p>策略梯度直接使用actor去跟环境互动，然后一个回合加得一个total reward 总奖励。策略梯度最大化整一个回合之后的total reward。而对同一个actor，total reward每次大概率不会相同，actor和environment都存在随机性，采样同样的action，每次的observation也会不一样。最大化total reward期望值，其可衡量actor的好坏，以下为计算过程。</p>
<p><img src="https://s2.loli.net/2024/05/18/i4dD1IAtvbsUVJg.png" alt="image.png"></p>
<p>对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>求导进行梯度优化，目标函数为total reward，调整<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>找到动作使total reward尽可能大</p>
<p><img src="https://s2.loli.net/2024/05/18/gT3biDMyfkm6xNU.png" alt="image.png"></p>
<p>使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi _{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>进行N次回合，得到轨迹，每条轨迹对应状态、动作和奖励</p>
<h3 id="策略梯度的公式简化">策略梯度的公式简化</h3>
<p>total reward 函数<br>
<img src="https://s2.loli.net/2024/05/18/usBW3brVZj7K6qz.png" alt="image.png"></p>
<p>total reward 导函数<br>
<img src="https://s2.loli.net/2024/05/18/QqMSjDgr64LCs3t.png" alt="image.png"></p>
<p>total reward 为正，则更新模型使得机率越大越好</p>
<p>总体流程</p>
<p><img src="https://s2.loli.net/2024/05/18/GrDQOYuFM7Jp2ta.png" alt="image.png"></p>
<h3 id="Trick1-Baseline">Trick1 Baseline</h3>
<p>当理想情况下，采样的轨迹的奖励大于零，鼓励该行为，但是可能某个行为没有被采样。</p>
<p><img src="https://s2.loli.net/2024/05/18/wfRYQV5ETKXAd19.png" alt="image.png"></p>
<p>平均归一化，使得total reward小于0，对应轨迹包含的行为减小概率</p>
<h3 id="Trick2-Suitable-Credit">Trick2 Suitable Credit</h3>
<p>在不同的action前面乘上不同的weight，增加折扣因子。</p>
<p>衡量对错，策略梯度采样蒙特卡洛的G值，让智能体一直走到最后，然后通过回溯计算G值</p>
<h2 id="Actor-Critic">Actor-Critic</h2>
<p>更好的权重更新方式，在t时刻状态选择某个action，进行打分（评价reward），则该评分函数称为critic</p>
<p>Actor网络和Critic网络的共同点：都输入状态S。输出策略以选择动作，称为actor(policy-based);计算每个动作分数（Q值），称为critic(valued-based)</p>
<p><img src="https://s2.loli.net/2024/05/19/o1yBiN5f9b82h3r.png" alt="image.png"></p>
<h2 id="Actor整合Critic">Actor整合Critic</h2>
<p>蒙特卡洛需要完成整个过程，到最终状态，才能回溯计算G值，效率较低。而使用时序差分（TD），不用回溯，每一步立即估算Q值。</p>
<p><img src="https://s2.loli.net/2024/05/19/9ECplxSkcNVdJ8s.png" alt="image.png"></p>
<p>Q值定义为action之后获得的奖励期望，Baseline是Q值求平均（Q值的期望），则Q值得期望正好为V值</p>
<ol>
<li>状态值函数V：使用动作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span>，状态s后得到累积奖励</li>
<li>状态动作值函数Q：使用动作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span>，状态s执行动作a后得到累积奖励</li>
</ol>
<h3 id="TD估算每一步的Q值，神经网络">TD估算每一步的Q值，神经网络</h3>
<h2 id="Advantage-Actor-Critic-A2C">Advantage Actor Critic(A2C)</h2>
<p>actor用Q(s,a)-V(s)指导更新，但两个网络估算Q和V，风险更大。</p>
<p>Q(s,a)用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>+</mo><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r+gamma*V(s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">amma</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>代替，将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>+</mo><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r+gamma*V(s&#x27;)-V(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">amma</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span>这个差作为TD-error</p>
<p>TD-error采用V，更新Critic的loss，而DQN中采用Q进行更新</p>
<p><img src="https://s2.loli.net/2024/05/19/GyR4cmThiHvqKQg.png" alt="image.png"></p>
<h3 id="Trick-1-shared-parameters">Trick 1 shared parameters</h3>
<p>对状态信息进行共享输入特征提取，即在输入到Critic网络和Actor网络之间，加入的一层网络</p>
<h3 id="Trick-2-修改reward">Trick 2 修改reward</h3>
<p>希望智能体在濒死状态下“力挽狂澜”，进行更多尝试。将reward减去20，表示在濒死状态下对选择动作a的不认同，即减去20以大大降低动作a出现的概率。同时reward向前传播，避免进入濒死状态，不选择进入濒死状态的动作</p>
<h2 id="Asynchronous-Advantage-Actor-Critic-A3C">Asynchronous Advantage Actor-Critic(A3C)</h2>
<p>基于异步和分布式思想，有多个智能体与环境交互，每个智能体都产生数据，进行学习。</p>
<p><img src="https://s2.loli.net/2024/05/19/JAbjEB3xon1QV7r.png" alt="image.png"></p>
<p>Global network 和 worker都为AC网络，worker不仅和环境互动，产生数据，并将自己从数据里计算到的梯度，汇总给全局网络(global networl)，应用在全局网络参数进行更新，而不是worker自己探索出来的数据。</p>
<h2 id="deep-deterministic-policy-gradient-DDPG">deep deterministic policy gradient(DDPG)</h2>
<p>DDPG输出的直接为一个动作，用一个actor去弥补DQN不能处理连续性控制问题的缺点。因为DQN中的maxQ(s’,a’)只能处理离散型</p>
<p>DQN中神经网络解决Qlearning不能解决连续状态空间问题，DDPG中神经网络解决DQN不能解决连续动作空间问题。</p>
<p>DDPG中actor功能：输入状态s,magic函数返回动作action的取值，使得Critic输出的Q值最大。即某个状态下，选择某个动作值时能获得的Q值。</p>
<p>DDPG并不是PG，没有带权重的梯度更新，而是梯度上升寻找Q的最大值。</p>
<ol>
<li>DDPG的Critic网络的作用是预估Q值，与AC中的Critic不同。Critic的输入有动作和状态。Critic的loss跟AC一样，TD-error</li>
<li>DDPG的actor是输出一个动作，该动作A输入到Critic后，能最大化Q值。DDPG中的actor的更新方式与AC中的不同，不是带权值的梯度更新，而是梯度上升最大化Q值。Q值最大化为目标函数</li>
</ol>
<p>DDPG和DQN一样，也采用固定网络(Fix Network)，先保持用来求target的网络，在更新之后，把参数赋值到target网络中。实际需要四个网络：actor critic actor-target critic-target</p>
<h2 id="Twin-Delayed-Deep-Deterministic-policy-gradient-algorithm-TD3-双延迟深度确定性策略梯度，DDPG的优化版本">(Twin Delayed Deep Deterministic policy gradient algorithm)TD3 双延迟深度确定性策略梯度，DDPG的优化版本</h2>
<h2 id="Double-Network">Double Network</h2>
<p>DDPG也存在Q值被高估的问题：argmaxQ(s’)代替V(s’)，去评估Q(S)。TD3则是通过选择相对较小的作为更新的目标。</p>
<p>TD3需要6个网络，目标网络中，估算出来的Q值用min()函数求出较少值，以这个值作为更新的目标，这个目标会更新两个网络Critic1和Critic2，两个网络是完全独立的，学习好的网络赋值给目标网络。</p>
<ol>
<li>Critic网络学习：计算Critic更新目标，才用target network，包括一个policy network，用于计算A’；两个Q network，计算两个Q值：Q1(A’)和Q2(A’)。Q1(A’)和Q2(A’)取最小值min(Q1,Q2)，代替DDPG的Q(a’)计算更新目标，target=min(Q1,Q2)*gamma+r。由于网络参数初值不同，导致计算值不同，有空间选择较小的值估算Q值，避免被高估。</li>
<li>Actor网络，梯度上升以学习Q值最高的action。actor并不在乎Q值被高估</li>
</ol>
<h2 id="Delayed-Actor-Update">Delayed Actor Update</h2>
<p>actor更新的delay，相对于critic更新多次后，actor再进行更新。</p>
<h2 id="target-policy-smoothing-regularization">target policy smoothing regularization</h2>
<p>TD3，给actor目标网络输出的action增加noise。给actor进行进行更多的探索，为给后面的critic目标网络增加难度。即在有输入干扰情况下，也给出正确分值，critic变得更加robust，相当于数据增强，提高模型的泛化能力</p>
<h2 id="Proximal-Policy-Optimization-PPO">Proximal Policy Optimization(PPO)</h2>
<p>AC架构可解决连续动作空间问题。</p>
<h3 id="AC输出连续动作">AC输出连续动作</h3>
<p>离散和连续动作输出</p>
<p><img src="https://s2.loli.net/2024/05/20/ZcdEijl57yeg4D3.png" alt="image.png"></p>
<h3 id="On-policy和Off-policy">On-policy和Off-policy</h3>
<p>策略梯度PG是在线策略，用于产生数据的策略（行为策略），与需要更新的策略（目标策略）是一致的</p>
<p>DQN是离线策略，让智能体与环境互动一定次数，获得数据。用这些数据优化策略后，继续跑新的数据，但以前的数据仍可以用。产生数据的策略和更新的目标策略不是同一个策略。</p>
<h3 id="Important-Sampling">Important Sampling</h3>
<p>PPO离线更新策略，重要采样技术。TD-error乘以重要性权重</p>
<p>PPO适用范围很广，较为稳定</p>
<p>重要性权重(IW)：目标策略（更新策略的网络）出现动作a的概率 除以 行为策略（产生数据的网络）出现a的概率</p>
<h3 id="Issue-of-Importance-Sampling">Issue of Importance Sampling</h3>
<p>即使乘以importance weight 对Q的期望u进行修正，方差可能不同。采样次数不多，方差不一样，差距越大，则采样可能会得到非常大的差距。</p>
<h3 id="Add-Constraint">Add Constraint</h3>
<p>限制两个分布差距不能太大</p>
<p>PPO1中使用KL散度（相对熵）衡量两个分布的差距。 KL散度为一种衡量两个概率分布的匹配程度的指标，两个分布差异越大，KL散度越大。在PPO1中作为惩罚项</p>
<p>KL计算参数使得行为action表现上面的距离，即策略的距离。策略就是action上面的几率分布。</p>
<p><img src="https://s2.loli.net/2024/05/20/pVv2AkyYLjifJha.png" alt="image.png"></p>
<h2 id="TRPO">TRPO</h2>
<p>KL散度作为约束项</p>
<h2 id="PPO2">PPO2</h2>
<p>直接裁剪更新的范围，比较两项中更小的。</p>
<h2 id="DPPO">DPPO</h2>
<p>A3C中需要worker跟环境进行互动，用产生的数据计算梯度，再调整全局网络中的参数。AC在线算法，产生数据的策略和更新的策略为同一个网络。不能将worker产出的数据，直接给全局网络计算梯度使用。</p>
<p>PPO解决离线更新策略问题，DPPO的worker只需要提供数据给全局网络，由全局网络从数据中直接学习。</p>
<p>DPPO中使用多线程学习，当全局网络学习时，workers需要等待全局网络学习完，才能干活；workers干活时，全局网络需要等待worker提供数据</p>
<p>比如用两个事件，解决两组不同线程的通信问题。我们有两组线程A，B，要求当A运行时，B等待；当A运行完毕，B开始运行，A等待。虽然有两组，只要线程的功能和需求相同，可将线程看成一组，由一组开关进行协调。</p>
<p>PPO图示</p>
<p><img src="https://s2.loli.net/2024/05/21/R9EL6fGlTbnYH4u.png" alt="image.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://LucienLSA.github.io">Shenao Lu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://lucienlsa.github.io/2024/04/29/RL/RL%E6%95%99%E7%A8%8Bb%E7%AB%99/">https://lucienlsa.github.io/2024/04/29/RL/RL%E6%95%99%E7%A8%8Bb%E7%AB%99/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://LucienLSA.github.io" target="_blank">Lucien</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/algorithm/">algorithm</a><a class="post-meta__tags" href="/tags/RL/">RL</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2024/04/05/Ivzr23MiNpQftUF.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/02/Python/PythonNotes/" title="Python使用记录"><img class="cover" src="https://s2.loli.net/2024/04/05/gPSBpWH4vMnhU6k.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python使用记录</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/25/ADP/parallel_OTC/" title="Parallel Control for Optimal Tracking via Adaptive Dynamic Programming"><img class="cover" src="https://s2.loli.net/2024/04/05/p45RjbaNBt2nioF.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Parallel Control for Optimal Tracking via Adaptive Dynamic Programming</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/04/08/ADP/CT_UnknownSys/" title="Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics"><img class="cover" src="https://s2.loli.net/2024/04/05/JC6P4ycwDeEKRfl.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-08</div><div class="title">Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics</div></div></a></div><div><a href="/2024/04/11/ADP/ET_CT/" title="Event-Triggered Adaptive Dynamic Programming for Continuous-Time Systems With Control Constraints"><img class="cover" src="https://s2.loli.net/2024/04/05/wLcfd8146Pilrt9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-11</div><div class="title">Event-Triggered Adaptive Dynamic Programming for Continuous-Time Systems With Control Constraints</div></div></a></div><div><a href="/2024/04/17/ADP/ET_TC_PUnknownConstrainedUncertain/" title="Event-Triggered ADP for Tracking Control of Partially Unknown Constrained Uncertain Systems"><img class="cover" src="https://s2.loli.net/2024/04/05/Ivzr23MiNpQftUF.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-17</div><div class="title">Event-Triggered ADP for Tracking Control of Partially Unknown Constrained Uncertain Systems</div></div></a></div><div><a href="/2024/04/12/ADP/ET_basic/" title="事件触发控制"><img class="cover" src="https://s2.loli.net/2024/04/05/JC6P4ycwDeEKRfl.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-12</div><div class="title">事件触发控制</div></div></a></div><div><a href="/2024/04/09/ADP/ET_DT/" title="Event-Triggered Adaptive Critic Control Design for Discrete-Time Constrained Nonlinear Systems"><img class="cover" src="https://s2.loli.net/2024/04/05/jIvfgaOuiDtSnz9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-09</div><div class="title">Event-Triggered Adaptive Critic Control Design for Discrete-Time Constrained Nonlinear Systems</div></div></a></div><div><a href="/2024/04/25/ADP/Model_Free_Track/" title="Model-Free Q-Learning for the Tracking Problem of Linear Discrete-Time Systems"><img class="cover" src="https://s2.loli.net/2024/04/05/Bd6znO2I8ZLceFD.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-25</div><div class="title">Model-Free Q-Learning for the Tracking Problem of Linear Discrete-Time Systems</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2024/04/04/oghnOyU3GQpPtKL.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shenao Lu</div><div class="author-info__description">永远喜欢并相信鲜花，诗歌，黄昏，音乐的永恒性，就算在浓稠的寒冬里，也能随时随地用它们栽种出一个春天。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">没有你在我有多难熬，没有你烦我有多烦恼。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">强化学习算法介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%86%E9%A2%91%E6%9D%A5%E6%BA%90"><span class="toc-number">1.1.</span> <span class="toc-text">视频来源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">基础介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"><span class="toc-number">1.3.</span> <span class="toc-text">马尔可夫链</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q%E5%80%BC%E5%92%8CV%E5%80%BC"><span class="toc-number">1.4.</span> <span class="toc-text">Q值和V值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#V%E5%80%BC"><span class="toc-number">1.4.1.</span> <span class="toc-text">V值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%80%BC"><span class="toc-number">1.4.2.</span> <span class="toc-text">Q值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#V%E5%80%BC%E5%92%8CQ%E5%80%BC"><span class="toc-number">1.4.3.</span> <span class="toc-text">V值和Q值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7"><span class="toc-number">1.5.</span> <span class="toc-text">蒙特卡洛采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%BC%BA%E7%82%B9"><span class="toc-number">1.5.1.</span> <span class="toc-text">蒙特卡洛缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC"><span class="toc-number">1.6.</span> <span class="toc-text">蒙特卡洛估算状态V值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GBDT%E7%AE%97%E6%B3%95-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.6.1.</span> <span class="toc-text">GBDT算法(梯度提升决策树)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Boosting%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">Boosting核心思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT%E5%8E%9F%E7%90%86"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">GBDT原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E6%A2%AF%E5%BA%A6%E8%BF%91%E4%BC%BC%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.6.1.3.</span> <span class="toc-text">负梯度近似误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.6.1.4.</span> <span class="toc-text">梯度提升与梯度下降</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B0%E5%B9%B3%E5%9D%87-%E6%97%A7%E5%B9%B3%E5%9D%87-%E6%AD%A5%E9%95%BF-%E6%96%B0%E5%8A%A0%E5%85%A5%E5%85%83%E7%B4%A0-%E6%97%A7%E5%B9%B3%E5%9D%87"><span class="toc-number">1.6.2.</span> <span class="toc-text">新平均&#x3D;旧平均+步长*(新加入元素-旧平均)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TD-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86-%E4%BC%B0%E7%AE%97%E7%8A%B6%E6%80%81V%E5%80%BC"><span class="toc-number">1.7.</span> <span class="toc-text">TD(时序差分)估算状态V值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E4%BC%B0%E7%AE%97V%E5%80%BC"><span class="toc-number">1.7.1.</span> <span class="toc-text">TD估算V值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SARSA%E7%AE%97%E6%B3%95"><span class="toc-number">1.8.</span> <span class="toc-text">SARSA算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SARSA%E6%80%9D%E6%83%B3"><span class="toc-number">1.8.1.</span> <span class="toc-text">SARSA思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SARSA-LambdaTable"><span class="toc-number">1.8.2.</span> <span class="toc-text">SARSA LambdaTable</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Qlearning%E7%AE%97%E6%B3%95"><span class="toc-number">1.9.</span> <span class="toc-text">Qlearning算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-table"><span class="toc-number">1.10.</span> <span class="toc-text">Q table</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DQN"><span class="toc-number">1.11.</span> <span class="toc-text">DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AAGreedy"><span class="toc-number">1.12.</span> <span class="toc-text">贪婪Greedy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E6%94%BE%E7%BC%93%E5%AD%98%EF%BC%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%EF%BC%89Replay-Buffer"><span class="toc-number">1.13.</span> <span class="toc-text">回放缓存（经验回放）Replay Buffer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fixed-Q-targets"><span class="toc-number">1.14.</span> <span class="toc-text">Fixed Q-targets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Double-DQN"><span class="toc-number">1.15.</span> <span class="toc-text">Double DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dueling-DQN"><span class="toc-number">1.16.</span> <span class="toc-text">Dueling DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prioritized-Experience-Replay"><span class="toc-number">1.17.</span> <span class="toc-text">Prioritized Experience Replay</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-step"><span class="toc-number">1.18.</span> <span class="toc-text">Multi-step</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Noise-Net"><span class="toc-number">1.19.</span> <span class="toc-text">Noise Net</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Noise-on-Action-Epsilon-Greedy"><span class="toc-number">1.19.1.</span> <span class="toc-text">Noise on Action(Epsilon Greedy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Noise-on-Parameters"><span class="toc-number">1.19.2.</span> <span class="toc-text">Noise on Parameters</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Distributional-Q-function"><span class="toc-number">1.20.</span> <span class="toc-text">Distributional Q-function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Continuous-Actions"><span class="toc-number">1.21.</span> <span class="toc-text">Continuous Actions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Solution1"><span class="toc-number">1.21.1.</span> <span class="toc-text">Solution1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solution2"><span class="toc-number">1.21.2.</span> <span class="toc-text">Solution2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Solution3"><span class="toc-number">1.21.3.</span> <span class="toc-text">Solution3</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Value-based-and-Policy-based"><span class="toc-number">1.22.</span> <span class="toc-text">Value-based and Policy-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-PG"><span class="toc-number">1.23.</span> <span class="toc-text">策略梯度(PG)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">1.23.1.</span> <span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%8E%9F%E7%90%86"><span class="toc-number">1.23.2.</span> <span class="toc-text">策略梯度原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%85%AC%E5%BC%8F%E7%AE%80%E5%8C%96"><span class="toc-number">1.23.3.</span> <span class="toc-text">策略梯度的公式简化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trick1-Baseline"><span class="toc-number">1.23.4.</span> <span class="toc-text">Trick1 Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trick2-Suitable-Credit"><span class="toc-number">1.23.5.</span> <span class="toc-text">Trick2 Suitable Credit</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic"><span class="toc-number">1.24.</span> <span class="toc-text">Actor-Critic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor%E6%95%B4%E5%90%88Critic"><span class="toc-number">1.25.</span> <span class="toc-text">Actor整合Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TD%E4%BC%B0%E7%AE%97%E6%AF%8F%E4%B8%80%E6%AD%A5%E7%9A%84Q%E5%80%BC%EF%BC%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.25.1.</span> <span class="toc-text">TD估算每一步的Q值，神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advantage-Actor-Critic-A2C"><span class="toc-number">1.26.</span> <span class="toc-text">Advantage Actor Critic(A2C)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Trick-1-shared-parameters"><span class="toc-number">1.26.1.</span> <span class="toc-text">Trick 1 shared parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trick-2-%E4%BF%AE%E6%94%B9reward"><span class="toc-number">1.26.2.</span> <span class="toc-text">Trick 2 修改reward</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-number">1.27.</span> <span class="toc-text">Asynchronous Advantage Actor-Critic(A3C)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deep-deterministic-policy-gradient-DDPG"><span class="toc-number">1.28.</span> <span class="toc-text">deep deterministic policy gradient(DDPG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Twin-Delayed-Deep-Deterministic-policy-gradient-algorithm-TD3-%E5%8F%8C%E5%BB%B6%E8%BF%9F%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%8CDDPG%E7%9A%84%E4%BC%98%E5%8C%96%E7%89%88%E6%9C%AC"><span class="toc-number">1.29.</span> <span class="toc-text">(Twin Delayed Deep Deterministic policy gradient algorithm)TD3 双延迟深度确定性策略梯度，DDPG的优化版本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Double-Network"><span class="toc-number">1.30.</span> <span class="toc-text">Double Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Delayed-Actor-Update"><span class="toc-number">1.31.</span> <span class="toc-text">Delayed Actor Update</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#target-policy-smoothing-regularization"><span class="toc-number">1.32.</span> <span class="toc-text">target policy smoothing regularization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Proximal-Policy-Optimization-PPO"><span class="toc-number">1.33.</span> <span class="toc-text">Proximal Policy Optimization(PPO)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AC%E8%BE%93%E5%87%BA%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C"><span class="toc-number">1.33.1.</span> <span class="toc-text">AC输出连续动作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-policy%E5%92%8COff-policy"><span class="toc-number">1.33.2.</span> <span class="toc-text">On-policy和Off-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Important-Sampling"><span class="toc-number">1.33.3.</span> <span class="toc-text">Important Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Issue-of-Importance-Sampling"><span class="toc-number">1.33.4.</span> <span class="toc-text">Issue of Importance Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-Constraint"><span class="toc-number">1.33.5.</span> <span class="toc-text">Add Constraint</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TRPO"><span class="toc-number">1.34.</span> <span class="toc-text">TRPO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO2"><span class="toc-number">1.35.</span> <span class="toc-text">PPO2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DPPO"><span class="toc-number">1.36.</span> <span class="toc-text">DPPO</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/27/OperationNetwork/nginx/" title="nginx学习与部署">nginx学习与部署</a><time datetime="2024-06-27T13:12:45.330Z" title="发表于 2024-06-27 21:12:45">2024-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/27/OperationNetwork/apache/" title="apache学习与部署">apache学习与部署</a><time datetime="2024-06-27T13:11:07.265Z" title="发表于 2024-06-27 21:11:07">2024-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/27/OperationNetwork/ansible/" title="ansible学习与部署">ansible学习与部署</a><time datetime="2024-06-27T13:09:35.214Z" title="发表于 2024-06-27 21:09:35">2024-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/27/OperationNetwork/zabbix/" title="zabbix学习与部署">zabbix学习与部署</a><time datetime="2024-06-27T13:06:26.644Z" title="发表于 2024-06-27 21:06:26">2024-06-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/27/OperationNetwork/jumpserver/" title="jumpserver学习与部署">jumpserver学习与部署</a><time datetime="2024-06-27T12:27:04.203Z" title="发表于 2024-06-27 20:27:04">2024-06-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Shenao Lu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>